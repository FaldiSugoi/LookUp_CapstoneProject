{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d54fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo pip install flask\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6181b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request\n",
    "import requests\n",
    "\n",
    "# Load labels\n",
    "from io import BytesIO\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "# For Dataset\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "\n",
    "# For Model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import ZeroPadding2D, Convolution2D, MaxPooling2D, Dropout, Flatten, Activation, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Resizing for tf shape\n",
    "import einops\n",
    "\n",
    "# Save result for later use\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6388a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.kaggle.com/timesler/facial-recognition-model-in-pytorch\n",
    "class DetectionPipeline:\n",
    "    \"\"\"Pipeline class for detecting faces in the frames of a video file.\"\"\"\n",
    "    \n",
    "    def __init__(self, detector, n_frames=None, batch_size=128, resize=None):\n",
    "        \"\"\"Constructor for DetectionPipeline class.\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            n_frames {int} -- Total number of frames to load. These will be evenly spaced\n",
    "                throughout the video. If not specified (i.e., None), all frames will be loaded.\n",
    "                (default: {None})\n",
    "            batch_size {int} -- Batch size to use with MTCNN face detector. (default: {32})\n",
    "            resize {float} -- Fraction by which to resize frames from original prior to face\n",
    "                detection. A value less than 1 results in downsampling and a value greater than\n",
    "                1 result in upsampling. (default: {None})\n",
    "        \"\"\"\n",
    "        self.detector = detector\n",
    "        self.n_frames = n_frames\n",
    "        self.batch_size = batch_size\n",
    "        self.resize = resize\n",
    "    \n",
    "    def __call__(self, filename):\n",
    "        \"\"\"Load frames from an MP4 video and detect faces.\n",
    "\n",
    "        Arguments:\n",
    "            filename {str} -- Path to video.\n",
    "        \"\"\"\n",
    "        # Create video reader and find length\n",
    "        v_cap = cv2.VideoCapture(filename)\n",
    "        v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Pick 'n_frames' evenly spaced frames to sample\n",
    "        if self.n_frames is None:\n",
    "            sample = np.arange(0, v_len)\n",
    "        else:\n",
    "            sample = np.linspace(0, v_len - 1, self.n_frames).astype(int)\n",
    "\n",
    "        # Loop through frames\n",
    "        faces = []\n",
    "        frames = []\n",
    "        for j in range(v_len):\n",
    "            success = v_cap.grab()\n",
    "            if j in sample:\n",
    "                # Load frame\n",
    "                success, frame = v_cap.retrieve()\n",
    "                if not success:\n",
    "                    continue\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = Image.fromarray(frame)\n",
    "                \n",
    "                # Resize frame to desired size\n",
    "                if self.resize is not None:\n",
    "                    frame = frame.resize([int(d * self.resize) for d in frame.size])\n",
    "                frames.append(frame)\n",
    "\n",
    "                # When batch is full, detect faces and reset frame list\n",
    "                if len(frames) % self.batch_size == 0 or j == sample[-1]:\n",
    "                    faces.extend(self.detector(frames))\n",
    "                    frames = []\n",
    "\n",
    "        v_cap.release()\n",
    "\n",
    "        return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3efea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.kaggle.com/timesler/facial-recognition-model-in-pytorch\n",
    "def process_faces(faces, feature_extractor):\n",
    "    \n",
    "    # Filter out frames without faces\n",
    "    faces = [f for f in faces if f is not None]\n",
    "    if len(faces) == 0 or faces == None:\n",
    "        return None\n",
    "    \n",
    "    embeddings = []\n",
    "    for i, face in zip(range(len(faces)), faces):\n",
    "        # Return a handful of faces\n",
    "        if i==25:\n",
    "            break\n",
    "        \n",
    "        # Transforming image to what TF wants\n",
    "        face = einops.rearrange(face,'b c w h -> b w h c')\n",
    "        #face = einops.rearrange(face, 'b c w h -> b (c h w)')\n",
    "        \n",
    "        # Extract the features\n",
    "        feature = feature_extractor(tf.cast(face, tf.float32))\n",
    "        embeddings.append(feature)\n",
    " \n",
    "    # Calculate centroid for video and distance of each face's feature vector from centroid\n",
    "    centroid = [sum(embed)/len(embeddings) for embed in zip(*embeddings)]\n",
    "    \n",
    "    distances = []\n",
    "    for embed in embeddings:\n",
    "        distance = [a-b for a,b in zip(embed, centroid)]\n",
    "        \n",
    "        # Normalize distance\n",
    "        y = tf.linalg.normalize(distance[3])\n",
    "        y = tf.convert_to_tensor(y[0])\n",
    "        #print(6)\n",
    "        # Scale values of tensor to be between 0 and 1\n",
    "        y = tf.truediv(\n",
    "            tf.subtract(\n",
    "                y,\n",
    "                tf.reduce_min(y)\n",
    "            ),\n",
    "            tf.subtract(\n",
    "                tf.reduce_max(y),\n",
    "                tf.reduce_min(y)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Global average pooling to reduce to face shape for training\n",
    "        #dist = einops.reduce(y,'b w h c -> h w', 'mean')\n",
    "        \n",
    "        # Convert back to what Keras wants\n",
    "        #dist = einops.rearrange(y,'b w h c -> b c w h')\n",
    "        #for x in dist:\n",
    "        #    distances.append(x)\n",
    "        #distances.append(dist)\n",
    "        \n",
    "        for x in y:\n",
    "            distances.append(x)\n",
    "        \n",
    "    #distances = np.asarray(distances).astype('float32')\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b44e305d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d (ZeroPadding2 (None, 162, 162, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 160, 160, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 80, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 82, 82, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 80, 80, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 42, 42, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 40, 40, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 22, 22, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 20, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 10, 10, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 1, 1, 4096)        52432896  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 1, 4096)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 1, 1, 4096)        16781312  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 1, 4096)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 1, 1, 2622)        10742334  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2622)              0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2622)              0         \n",
      "=================================================================\n",
      "Total params: 83,867,326\n",
      "Trainable params: 83,867,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model for original frames\n",
    "# From the paper https://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf\n",
    "\n",
    "model_frames = keras.Sequential()\n",
    "model_frames.add(ZeroPadding2D((1,1),input_shape=(160,160, 3)))\n",
    "model_frames.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "#model_frames.add(ZeroPadding2D((1,1)))\n",
    "#model_frames.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model_frames.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model_frames.add(ZeroPadding2D((1,1)))\n",
    "model_frames.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "#model_frames.add(ZeroPadding2D((1,1)))\n",
    "#model_frames.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model_frames.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model_frames.add(ZeroPadding2D((1,1)))\n",
    "model_frames.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "#model_frames.add(ZeroPadding2D((1,1)))\n",
    "#model_frames.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "#model_frames.add(ZeroPadding2D((1,1)))\n",
    "#model_frames.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model_frames.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model_frames.add(ZeroPadding2D((1,1)))\n",
    "model_frames.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "#model_frames.add(ZeroPadding2D((1,1)))\n",
    "#model_frames.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "#model_frames.add(ZeroPadding2D((1,1)))\n",
    "#model_frames.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model_frames.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model_frames.add(ZeroPadding2D((1,1)))\n",
    "model_frames.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "#model_frames.add(ZeroPadding2D((1,1)))\n",
    "#model_frames.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "#model_frames.add(ZeroPadding2D((1,1)))\n",
    "#model_frames.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model_frames.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model_frames.add(Convolution2D(4096, (5, 5), activation='relu'))\n",
    "model_frames.add(Dropout(0.5))\n",
    "model_frames.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "model_frames.add(Dropout(0.5))\n",
    "model_frames.add(Convolution2D(2622, (1, 1)))\n",
    "model_frames.add(Flatten())\n",
    "model_frames.add(Activation('softmax'))\n",
    "\n",
    "model_frames.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc7fa6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = keras.Model(\n",
    "    inputs=model_frames.inputs,\n",
    "    outputs=[layer.output for layer in model_frames.layers],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a36c6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load face detector\n",
    "face_detector = MTCNN(margin=14, keep_all=True, factor=0.).eval()\n",
    "\n",
    "# Load facial recognition model\n",
    "# feature_extractor = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "# Define face detection pipeline\n",
    "detection_pipeline = DetectionPipeline(detector=face_detector, n_frames=None, batch_size=128, resize=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e25e1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model into object\n",
    "model = load_model('model-e10-24.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bb9a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      " * Running on http://10.148.0.2:5000/ (Press CTRL+C to quit)\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:47: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "34.87.131.25 - - [08/Jun/2021 13:05:54] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed): 1.29e+02\r"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return \"Hello, world!\"\n",
    "\n",
    "# Create /predict endpoint with POST method\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    request_json = request.json\n",
    "#     print(\"data: {}\").format(request_json)\n",
    "#     print(\"type: {}\").format(type(request_json))\n",
    "    \n",
    "    # Predict from 'data' field in request body,\n",
    "    # and return prediction in response body\n",
    "    \n",
    "    vid = request_json.get('videoUrl')\n",
    "\n",
    "    X = []\n",
    "    start = time.time()\n",
    "    n_processed = 0\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "\n",
    "            # Load frames and find faces\n",
    "            faces = detection_pipeline(vid)\n",
    "\n",
    "            features = process_faces(faces, feature_extractor)\n",
    "\n",
    "            if features != None:\n",
    "                for z in features:\n",
    "                    assert not np.any(np.isnan(z))\n",
    "                    X.append(z)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print('\\nStopped.')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "            \n",
    "        n_processed += len(faces)\n",
    "        print(f'Frames per second (load+detect+embed): {n_processed / (time.time() - start):6.3}\\r', end='')\n",
    "        \n",
    "        # Convert to array for prediction\n",
    "        X = np.asarray(X).astype('float32')\n",
    "\n",
    "    if X != None:\n",
    "        prediction = model.predict(X)\n",
    "        prediction_string = str(prediction)\n",
    "    else:\n",
    "        prediction_string = [\"No face found\"]\n",
    "    \n",
    "    #prediction_string = [str(d) for d in prediction]\n",
    "    response_json = {\n",
    "        'data' : request_json.get('videoUrl'),\n",
    "        'prediction' : list(prediction_string)\n",
    "    }\n",
    "    \n",
    "    return json.dumps(response_json)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "    #app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a79757",
   "metadata": {},
   "source": [
    "##### Prediction label based using model.fit (manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load result into array\n",
    "file = open('X-24', 'rb')\n",
    "X = np.load(file, allow_pickle=True)\n",
    "file.close\n",
    "\n",
    "y = np.load('Y-24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57e2480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80:20 for split size\n",
    "size = int(len(X) * 0.8)\n",
    "\n",
    "x_train, x_test = X[:size], X[size:]\n",
    "y_train, y_test = y[:size], y[size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64e7f353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2419"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Free unused memories\n",
    "import gc\n",
    "del X\n",
    "del y\n",
    "del x_train\n",
    "del y_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cce8035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 24s 5s/step - loss: 0.5209 - accuracy: 0.7868\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "\n",
    "# Generate predictions\n",
    "# For example, generate predictions for 3 samples\n",
    "predictions = model.predict(x_test[:3])\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c97a897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7547928 ],\n",
       "       [0.75821173],\n",
       "       [0.75627834]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab94cee7",
   "metadata": {},
   "source": [
    "##### Prediction probabilities using face processing (X) manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df87817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction probability of face using X\n",
    "filenames = '../resources/dataset/test_videos/*.mp4'\n",
    "\n",
    "# Prediction\n",
    "bias = -0.4\n",
    "weight = 0.68235746\n",
    "\n",
    "probabilities = []\n",
    "for filename, x_i in zip(filenames, X):\n",
    "    if x_i is not None:\n",
    "        prob = 1 / (1 + np.exp(-(bias + (weight * x_i).mean())))\n",
    "    else:\n",
    "        prob = 0.5\n",
    "    probabilities.append([os.path.basename(filename), prob])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m69"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
