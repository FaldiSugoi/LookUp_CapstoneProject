{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3f2795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "num_cpu = multiprocessing.cpu_count()\n",
    "num_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd0403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Dataset\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "\n",
    "# For Model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import ZeroPadding2D, Convolution2D, MaxPooling2D, Dropout, Flatten, Activation, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Resizing for tf shape\n",
    "import einops\n",
    "\n",
    "# Save result for later use\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "\n",
    "#from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2facbf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.kaggle.com/timesler/facial-recognition-model-in-pytorch\n",
    "class DetectionPipeline:\n",
    "    \"\"\"Pipeline class for detecting faces in the frames of a video file.\"\"\"\n",
    "    \n",
    "    def __init__(self, detector, n_frames=None, batch_size=128, resize=None):\n",
    "        \"\"\"Constructor for DetectionPipeline class.\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            n_frames {int} -- Total number of frames to load. These will be evenly spaced\n",
    "                throughout the video. If not specified (i.e., None), all frames will be loaded.\n",
    "                (default: {None})\n",
    "            batch_size {int} -- Batch size to use with MTCNN face detector. (default: {32})\n",
    "            resize {float} -- Fraction by which to resize frames from original prior to face\n",
    "                detection. A value less than 1 results in downsampling and a value greater than\n",
    "                1 result in upsampling. (default: {None})\n",
    "        \"\"\"\n",
    "        self.detector = detector\n",
    "        self.n_frames = n_frames\n",
    "        self.batch_size = batch_size\n",
    "        self.resize = resize\n",
    "    \n",
    "    def __call__(self, filename):\n",
    "        \"\"\"Load frames from an MP4 video and detect faces.\n",
    "\n",
    "        Arguments:\n",
    "            filename {str} -- Path to video.\n",
    "        \"\"\"\n",
    "        # Create video reader and find length\n",
    "        v_cap = cv2.VideoCapture(filename)\n",
    "        v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Pick 'n_frames' evenly spaced frames to sample\n",
    "        if self.n_frames is None:\n",
    "            sample = np.arange(0, v_len)\n",
    "        else:\n",
    "            sample = np.linspace(0, v_len - 1, self.n_frames).astype(int)\n",
    "\n",
    "        # Loop through frames\n",
    "        faces = []\n",
    "        frames = []\n",
    "        for j in range(v_len):\n",
    "            success = v_cap.grab()\n",
    "            if j in sample:\n",
    "                # Load frame\n",
    "                success, frame = v_cap.retrieve()\n",
    "                if not success:\n",
    "                    continue\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = Image.fromarray(frame)\n",
    "                \n",
    "                # Resize frame to desired size\n",
    "                if self.resize is not None:\n",
    "                    frame = frame.resize([int(d * self.resize) for d in frame.size])\n",
    "                frames.append(frame)\n",
    "\n",
    "                # When batch is full, detect faces and reset frame list\n",
    "                if len(frames) % self.batch_size == 0 or j == sample[-1]:\n",
    "                    faces.extend(self.detector(frames))\n",
    "                    frames = []\n",
    "\n",
    "        v_cap.release()\n",
    "\n",
    "        return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a78b8e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.kaggle.com/timesler/facial-recognition-model-in-pytorch\n",
    "def process_faces(faces, feature_extractor):\n",
    "    \n",
    "    # Filter out frames without faces\n",
    "    faces = [f for f in faces if f is not None]\n",
    "    if len(faces) == 0 or faces == None:\n",
    "        return None\n",
    "    \n",
    "    embeddings = []\n",
    "    for i, face in zip(range(len(faces)), faces):\n",
    "        # Return a handful of faces\n",
    "        if i==25:\n",
    "            break\n",
    "        \n",
    "        # Transforming image to what TF wants\n",
    "        face = einops.rearrange(face,'b c w h -> b w h c')\n",
    "        #face = einops.rearrange(face, 'b c w h -> b (c h w)')\n",
    "        \n",
    "        # Extract the features\n",
    "        feature = feature_extractor(tf.cast(face, tf.float32))\n",
    "        embeddings.append(feature)\n",
    " \n",
    "    # Calculate centroid for video and distance of each face's feature vector from centroid\n",
    "    centroid = [sum(embed)/len(embeddings) for embed in zip(*embeddings)]\n",
    "    \n",
    "    distances = []\n",
    "    for embed in embeddings:\n",
    "        distance = [a-b for a,b in zip(embed, centroid)]\n",
    "        \n",
    "        # Normalize distance\n",
    "        y = tf.linalg.normalize(distance[3])\n",
    "        y = tf.convert_to_tensor(y[0])\n",
    "        #print(6)\n",
    "        # Scale values of tensor to be between 0 and 1\n",
    "        y = tf.truediv(\n",
    "            tf.subtract(\n",
    "                y,\n",
    "                tf.reduce_min(y)\n",
    "            ),\n",
    "            tf.subtract(\n",
    "                tf.reduce_max(y),\n",
    "                tf.reduce_min(y)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Global average pooling to reduce to face shape for training\n",
    "        #dist = einops.reduce(y,'b w h c -> h w', 'mean')\n",
    "        \n",
    "        # Convert back to what Keras wants\n",
    "        #dist = einops.rearrange(y,'b w h c -> b c w h')\n",
    "        #for x in dist:\n",
    "        #    distances.append(x)\n",
    "        #distances.append(dist)\n",
    "        \n",
    "        for x in y:\n",
    "            distances.append(x)\n",
    "        \n",
    "    #distances = np.asarray(distances).astype('float32')\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec5094ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d (ZeroPadding2 (None, 162, 162, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 160, 160, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 80, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 82, 82, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 80, 80, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 42, 42, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 40, 40, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 22, 22, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 20, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 10, 10, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 1, 1, 4096)        52432896  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 1, 4096)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 1, 1, 4096)        16781312  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 1, 4096)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 1, 1, 2622)        10742334  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2622)              0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2622)              0         \n",
      "=================================================================\n",
      "Total params: 83,867,326\n",
      "Trainable params: 83,867,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model for original frames\n",
    "# From the paper https://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf\n",
    "\n",
    "model_frames = keras.Sequential()\n",
    "model_frames.add(ZeroPadding2D((1,1),input_shape=(160,160, 3)))\n",
    "model_frames.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "#model_frames.add(ZeroPadding2D((1,1)))\n",
    "#model_frames.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model_frames.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model_frames.add(ZeroPadding2D((1,1)))\n",
    "model_frames.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "#model_frames.add(ZeroPadding2D((1,1)))\n",
    "#model_frames.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model_frames.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model_frames.add(ZeroPadding2D((1,1)))\n",
    "model_frames.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "#model_frames.add(ZeroPadding2D((1,1)))\n",
    "#model_frames.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "#model_frames.add(ZeroPadding2D((1,1)))\n",
    "#model_frames.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model_frames.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model_frames.add(ZeroPadding2D((1,1)))\n",
    "model_frames.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "#model_frames.add(ZeroPadding2D((1,1)))\n",
    "#model_frames.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "#model_frames.add(ZeroPadding2D((1,1)))\n",
    "#model_frames.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model_frames.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model_frames.add(ZeroPadding2D((1,1)))\n",
    "model_frames.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "#model_frames.add(ZeroPadding2D((1,1)))\n",
    "#model_frames.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "#model_frames.add(ZeroPadding2D((1,1)))\n",
    "#model_frames.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model_frames.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model_frames.add(Convolution2D(4096, (5, 5), activation='relu'))\n",
    "model_frames.add(Dropout(0.5))\n",
    "model_frames.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "model_frames.add(Dropout(0.5))\n",
    "model_frames.add(Convolution2D(2622, (1, 1)))\n",
    "model_frames.add(Flatten())\n",
    "model_frames.add(Activation('softmax'))\n",
    "\n",
    "model_frames.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbe6dc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d_5 (ZeroPaddin (None, 84, 84, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 21, 21, 64)        16777280  \n",
      "_________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPaddin (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 21, 21, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPaddin (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 10, 10, 128)       73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPaddin (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 10, 10, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 1, 1, 1024)        3277824   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 1, 1, 1024)        1049600   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 1, 1, 2622)        2687550   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2622)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 2623      \n",
      "=================================================================\n",
      "Total params: 24,053,245\n",
      "Trainable params: 24,053,245\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model_faces for detected faces\n",
    "\n",
    "model_faces = keras.Sequential()\n",
    "model_faces.add(ZeroPadding2D((1,1),input_shape=(82,82,64)))\n",
    "model_faces.add(Convolution2D(64, (64, 64), activation='relu'))\n",
    "model_faces.add(ZeroPadding2D((1,1)))\n",
    "model_faces.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model_faces.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model_faces.add(ZeroPadding2D((1,1)))\n",
    "model_faces.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model_faces.add(ZeroPadding2D((1,1)))\n",
    "model_faces.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model_faces.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model_faces.add(Convolution2D(1024, (5, 5), activation='relu'))\n",
    "model_faces.add(Dropout(0.5))\n",
    "model_faces.add(Convolution2D(1024, (1, 1), activation='relu'))\n",
    "model_faces.add(Dropout(0.5))\n",
    "model_faces.add(Convolution2D(2622, (1, 1)))\n",
    "model_faces.add(Flatten())\n",
    "model_faces.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_faces.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3df3e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = keras.Model(\n",
    "    inputs=model_frames.inputs,\n",
    "    outputs=[layer.output for layer in model_frames.layers],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec330e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load face detector\n",
    "face_detector = MTCNN(margin=14, keep_all=True, factor=0.).eval()\n",
    "\n",
    "# Load facial recognition model\n",
    "# feature_extractor = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "# Define face detection pipeline\n",
    "detection_pipeline = DetectionPipeline(detector=face_detector, n_frames=None, batch_size=128, resize=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2e97a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9777a68f234799a774d795c1f92f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/facenet_pytorch/models/utils/detect_face.py:183: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  batch_boxes, batch_points = np.array(batch_boxes), np.array(batch_points)\n",
      "/opt/conda/lib/python3.7/site-packages/facenet_pytorch/models/mtcnn.py:339: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  boxes = np.array(boxes)\n",
      "/opt/conda/lib/python3.7/site-packages/facenet_pytorch/models/mtcnn.py:341: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  points = np.array(points)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   28.7\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/facenet_pytorch/models/mtcnn.py:340: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  probs = np.array(probs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   27.5\n",
      "Frames per second (load+detect+embed):   26.1\n",
      "Frames per second (load+detect+embed):   26.3\n",
      "Incompatible shapes: [2,162,162,3] vs. [3,162,162,3] [Op:AddV2]\n",
      "Frames per second (load+detect+embed):   26.2\n",
      "Frames per second (load+detect+embed):   26.3\n",
      "Frames per second (load+detect+embed):   26.3\n",
      "Incompatible shapes: [3,162,162,3] vs. [2,162,162,3] [Op:AddV2]\n",
      "Frames per second (load+detect+embed):   26.2\r"
     ]
    }
   ],
   "source": [
    "with open('../resources/dataset/metadata.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "filenames = glob.glob('../resources/dataset/train_sample_videos/*.mp4')\n",
    "\n",
    "counter=0\n",
    "X = []\n",
    "y = []\n",
    "start = time.time()\n",
    "n_processed = 0\n",
    "with torch.no_grad():\n",
    "    for i, filename in tqdm(enumerate(filenames), total=len(filenames)):\n",
    "        #print(i, filename)\n",
    "\n",
    "        try:\n",
    "            # Load frames and find faces\n",
    "            faces = detection_pipeline(filename)\n",
    "\n",
    "            # Calculate distances of feature vectors\n",
    "            #X.append(process_faces(faces, feature_extractor))\n",
    "\n",
    "            features = process_faces(faces, feature_extractor)\n",
    "\n",
    "            #if (len(z)!=0):\n",
    "            if features != None:\n",
    "                for z in features:\n",
    "                    assert not np.any(np.isnan(z))\n",
    "                    #print(f\"z: {z}\")\n",
    "                    X.append(z)\n",
    "                    #print(f\"X: {X}\")\n",
    "                    if(data[filename[41:]]['label']=='FAKE'):\n",
    "                        y.append(1)\n",
    "                    else:\n",
    "                        y.append(0)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print('\\nStopped.')\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            #X.append(None)\n",
    "            #y.append(None)\n",
    "\n",
    "        n_processed += len(faces)\n",
    "        print(f'Frames per second (load+detect+embed): {n_processed / (time.time() - start):6.3}\\r', end='')\n",
    "\n",
    "        # Converting back and forth because does not have enough memory to convert huge array at one time\n",
    "        # Convert to array for split\n",
    "        #X = np.asarray(X).astype('float32')\n",
    "        #y = np.asarray(y)\n",
    "\n",
    "        # Convert it back if there are new features to append\n",
    "        #X = X.tolist()\n",
    "        #y = y.tolist()\n",
    "\n",
    "        if i==200:\n",
    "            # Convert to array for split\n",
    "            X = np.asarray(X).astype('float32')\n",
    "            y = np.asarray(y)\n",
    "\n",
    "            # Create a file if it is not exist\n",
    "            f = open('X-200', 'w')\n",
    "            f.close\n",
    "\n",
    "            f = open('Y-200', 'w')\n",
    "            f.close\n",
    "            \n",
    "            # Save result to a file\n",
    "            file = open('X-200', 'wb')\n",
    "            np.save(file, X)\n",
    "            file.close\n",
    "\n",
    "            file2 = open('Y-200', 'wb')\n",
    "            np.save(file2, y)\n",
    "            file2.close            \n",
    "        \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecaee4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = open('X-200', 'rb')\n",
    "X = np.load(result, allow_pickle=True)\n",
    "\n",
    "result2 = open('Y-200', 'rb')\n",
    "y = np.load(result2, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1e4333d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(554, 82, 82, 64) float32\n",
      "(17,) int64\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, X.dtype)\n",
    "print(y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e2a84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80:20 for split size\n",
    "size = int(len(X) * 0.8)\n",
    "\n",
    "x_train, x_test = X[:size], X[size:]\n",
    "y_train, y_test = y[:size], y[size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e581b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To clear memory\n",
    "import gc\n",
    "del model_frames\n",
    "del X\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ae0c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_faces.compile(\n",
    "    optimizer=SGD(learning_rate=0.01, momentum=0.9),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1df161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 554s 67s/step - loss: 0.6238 - accuracy: 0.7236\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 514s 62s/step - loss: 0.5430 - accuracy: 0.7969\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 516s 63s/step - loss: 0.5208 - accuracy: 0.7980\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 504s 61s/step - loss: 0.5231 - accuracy: 0.7980\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 486s 59s/step - loss: 0.5131 - accuracy: 0.7980\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 489s 60s/step - loss: 0.5239 - accuracy: 0.7980\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 494s 60s/step - loss: 0.5090 - accuracy: 0.7980\n",
      "Epoch 9/100\n",
      "5/8 [=================>............] - ETA: 3:25 - loss: 0.5476 - accuracy: 0.7922"
     ]
    }
   ],
   "source": [
    "history = model_faces.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    epochs=100,\n",
    "    use_multiprocessing=True,\n",
    "    workers=num_cpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb74cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model into a file\n",
    "model_faces.save('model.h5')\n",
    "#del model"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m69"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
